# Spark Fundamentals
 Đến ngày nay dữ liệu càng ngày càng nhiều, trong bối cảnh này, những phẩm chất nào là một khuôn khổ xử lý dữ liệu lý tưởng nên có?

 Nó phải có khả năng xử lý các khối dữ liệu được phân phối qua một cụm máy tính

 Nó có thể xử lý dữ liệu một cách song song để một công việc xử lý dữ liệu khổng lồ có thể được chia thành nhiều công việc được xử lý song song để thời gian xử lý có thể được giảm đáng kể

 Nó phải có khả năng sử dụng sức mạnh xử lý của tất cả các lõi hoặc bộ vi xử lý trong máy tính

 Nó phải có khả năng sử dụng tất cả các máy tính có sẵn trong một cluster Nó phải có khả năng chạy trên phần cứng hàng hóa

 Có hai khuôn khổ xử lý dữ liệu mã nguồn mở đáng chú ý là đáp ứng tất cả các yêu cầu này. Đầu tiên là Apache Hadoop và phần thứ hai là Apache Spark.

 ## An overview of Apache Hadoop

Apache Hadoop là một khuôn khổ phần mềm mã nguồn mở được thiết kế để tạo lưu trữ dữ liệu phân tán trên một cụm máy tính và để sử dụng.Khung này đi kèm với một hệ thống tập tin phân phối cho việc lưu trữ dữ liệu, tên,Hadoop Distributed File System (HDFS), khung sử lý dữ liệu, namely, MapReduce.
